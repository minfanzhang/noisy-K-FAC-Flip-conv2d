/h/minfanzh/noisy-K-FAC_use_all_FC/main.py
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
import numpy as np
import os

from misc.utils import get_logger, get_args, makedirs
from misc.config import process_config
from misc.data_loader import load_pytorch
from core.model import Model
from core.train import Trainer


_INPUT_DIM = {
    'fmnist': [784],
    'mnist': [784],
    'cifar10': [32, 32, 3],
    'cifar100': [32, 32, 3]
}


def main():
    tf.set_random_seed(1231)
    np.random.seed(1231)

    try:
        args = get_args()
        config = process_config(args.config)
    except:
        print("Add a config file using \'--config file_name.json\'")
        exit(1)

    makedirs(config.summary_dir)
    makedirs(config.checkpoint_dir)

    # set logger
    path = os.path.dirname(os.path.abspath(__file__))
    path1 = os.path.join(path, 'core/model.py')
    path2 = os.path.join(path, 'core/train.py')
    logger = get_logger('log', logpath=config.summary_dir+'/',
                        filepath=os.path.abspath(__file__), package_files=[path1, path2])

    logger.info(config)

    # load data
    train_loader, test_loader = load_pytorch(config)

    # define computational graph
    sess = tf.Session()

    model_ = Model(config, _INPUT_DIM[config.dataset], len(train_loader.dataset))
    trainer = Trainer(sess, model_, train_loader, test_loader, config, logger)

    trainer.train()


if __name__ == "__main__":
    main()

/h/minfanzh/noisy-K-FAC_use_all_FC/core/model.py
import tensorflow as tf

from ops import optimizer as opt
from ops import layer_collection as lc
from ops import sampler as sp
from network.registry import get_model
from core.base_model import BaseModel


class Model(BaseModel):
    def __init__(self, config, input_dim, n_data):
        super().__init__(config)
        self.layer_collection = lc.LayerCollection()
        self.input_dim = input_dim
        self.n_data = n_data

        self.cov_update_op = None
        self.inv_update_op = None

        self.build_model()
        self.init_optim()
        self.init_saver()

    @property
    def trainable_variables(self):
        # note: we don't train the params of BN
        vars = []
        for var in tf.trainable_variables():
            if "w" in var.name:
                vars.append(var)
        return vars

    def build_model(self):
        self.inputs = tf.placeholder(tf.float32, [None] + self.input_dim)
        self.targets = tf.placeholder(tf.int64, [None])
        self.is_training = tf.placeholder(tf.bool)
        self.n_particles = tf.placeholder(tf.int32)

        inputs = self.inputs
        net = get_model(self.config.model_name)

        self.sampler = sp.Sampler(self.config, self.n_data, self.n_particles)
        logits, l2_loss = net(inputs, self.sampler, self.is_training,
                              self.config.batch_norm, self.layer_collection,
                              self.n_particles)

        # ensemble
        logits_ = tf.reduce_mean(
            tf.reshape(logits, [self.n_particles, -1, tf.shape(logits)[-1]]), 0)
        self.acc = tf.reduce_mean(tf.cast(tf.equal(
            self.targets, tf.argmax(logits_, axis=1)), dtype=tf.float32))

        targets_ = tf.tile(self.targets, [self.n_particles])
        self.loss = tf.reduce_mean(
            tf.nn.sparse_softmax_cross_entropy_with_logits(
                labels=targets_, logits=logits))

        coeff = self.config.kl / (self.n_data * self.config.eta)
        self.total_loss = self.loss + coeff * l2_loss

    def init_optim(self):
        self.optim = opt.KFACOptimizer(var_list=self.trainable_variables,
                                       learning_rate=self.config.learning_rate,
                                       cov_ema_decay=self.config.cov_ema_decay,
                                       damping=self.config.damping,
                                       layer_collection=self.layer_collection,
                                       norm_constraint=tf.train.exponential_decay(self.config.kl_clip,
                                                                                  self.global_step_tensor,
                                                                                  390, 0.95, staircase=True),
                                       momentum=self.config.momentum)

        self.cov_update_op = self.optim.cov_update_op
        self.inv_update_op = self.optim.inv_update_op

        with tf.control_dependencies([self.inv_update_op]):
            self.var_update_op = self.sampler.update(self.layer_collection.get_blocks())

        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        with tf.control_dependencies(update_ops):
            self.train_op = self.optim.minimize(self.total_loss, global_step=self.global_step_tensor)

    def init_saver(self):
        self.saver = tf.train.Saver(max_to_keep=self.config.max_to_keep)


/h/minfanzh/noisy-K-FAC_use_all_FC/core/train.py
from core.base_train import BaseTrain
from tqdm import tqdm
import numpy as np


class Trainer(BaseTrain):
    def __init__(self, sess, model, train_loader, test_loader, config, logger):
        super(Trainer, self).__init__(sess, model, config, logger)
        self.train_loader = train_loader
        self.test_loader = test_loader

    def train(self):
        for cur_epoch in range(self.config.epoch):
            self.logger.info('epoch: {}'.format(int(cur_epoch)))
            self.train_epoch()
            self.test_epoch()

    def train_epoch(self):
        loss_list = []
        acc_list = []
        for itr, (x, y) in enumerate(tqdm(self.train_loader)):
            feed_dict = {
                self.model.inputs: x,
                self.model.targets: y,
                self.model.n_particles: self.config.train_particles
            }

            feed_dict.update({self.model.is_training: True})
            self.sess.run([self.model.train_op], feed_dict=feed_dict)

            feed_dict.update({self.model.is_training: False})  # note: that's important
            loss, acc = self.sess.run([self.model.loss, self.model.acc], feed_dict=feed_dict)
            loss_list.append(loss)
            acc_list.append(acc)

            cur_iter = self.model.global_step_tensor.eval(self.sess)
            if cur_iter % self.config.TCov == 0:
                self.sess.run([self.model.cov_update_op], feed_dict=feed_dict)

            if cur_iter % self.config.TInv == 0:
                self.sess.run([self.model.inv_update_op, self.model.var_update_op], feed_dict=feed_dict)

        avg_loss = np.mean(loss_list)
        avg_acc = np.mean(acc_list)
        self.logger.info("train | loss: %5.4f | accuracy: %5.4f"%(float(avg_loss), float(avg_acc)))

        # summarize
        summaries_dict = dict()
        summaries_dict['train_loss'] = avg_loss
        summaries_dict['train_acc'] = avg_acc

        # summarize
        cur_iter = self.model.global_step_tensor.eval(self.sess)
        self.summarizer.summarize(cur_iter, summaries_dict=summaries_dict)

        # self.model.save(self.sess)

    def test_epoch(self):
        loss_list = []
        acc_list = []
        for (x, y) in self.test_loader:
            feed_dict = {
                self.model.inputs: x,
                self.model.targets: y,
                self.model.is_training: False,
                self.model.n_particles: self.config.test_particles
            }
            loss, acc = self.sess.run([self.model.loss, self.model.acc], feed_dict=feed_dict)
            loss_list.append(loss)
            acc_list.append(acc)

        avg_loss = np.mean(loss_list)
        avg_acc = np.mean(acc_list)
        self.logger.info("test | loss: %5.4f | accuracy: %5.4f\n"%(float(avg_loss), float(avg_acc)))

        # summarize
        summaries_dict = dict()
        summaries_dict['test_loss'] = avg_loss
        summaries_dict['test_acc'] = avg_acc

        # summarize
        cur_iter = self.model.global_step_tensor.eval(self.sess)
        self.summarizer.summarize(cur_iter, summaries_dict=summaries_dict)

TCov: 10
TInv: 200
batch_norm: false
batch_size: 128
checkpoint_dir: ./experiments/cifar10/noisy-kfac/checkpoint/
cov_ema_decay: 0.99
damping: 0.001
data_aug: false
data_path: ../data
dataset: cifar10
epoch: 150
eta: 0.1
exp_name: noisy-kfac
fisher_approx: kron
kl: 0.5
kl_clip: 0.001
learning_rate: 0.0001
max_to_keep: 3
model_name: vgg16
momentum: 0.9
num_workers: 2
optimizer: kfac
summary_dir: ./experiments/cifar10/noisy-kfac/summary/
test_batch_size: 100
test_particles: 10
train_particles: 1

epoch: 0
train | loss: 2.2115 | accuracy: 0.1979
test | loss: 2.1135 | accuracy: 0.2373

epoch: 1
train | loss: 2.0872 | accuracy: 0.2561
test | loss: 2.0548 | accuracy: 0.2711

epoch: 2
train | loss: 2.0252 | accuracy: 0.2904
test | loss: 1.9912 | accuracy: 0.3021

epoch: 3
train | loss: 1.9551 | accuracy: 0.3197
test | loss: 1.9176 | accuracy: 0.3379

epoch: 4
train | loss: 1.8723 | accuracy: 0.3540
test | loss: 1.8359 | accuracy: 0.3714

epoch: 5
train | loss: 1.7820 | accuracy: 0.3849
test | loss: 1.7548 | accuracy: 0.3990

epoch: 6
train | loss: 1.6941 | accuracy: 0.4112
test | loss: 1.6888 | accuracy: 0.4259

epoch: 7
train | loss: 1.6187 | accuracy: 0.4343
test | loss: 1.6441 | accuracy: 0.4460

epoch: 8
train | loss: 1.5600 | accuracy: 0.4528
test | loss: 1.6226 | accuracy: 0.4611

epoch: 9
train | loss: 1.5199 | accuracy: 0.4648
test | loss: 1.6158 | accuracy: 0.4713

epoch: 10
train | loss: 1.4864 | accuracy: 0.4768
test | loss: 1.6200 | accuracy: 0.4799

epoch: 11
train | loss: 1.4638 | accuracy: 0.4864
test | loss: 1.6322 | accuracy: 0.4863

epoch: 12
train | loss: 1.4515 | accuracy: 0.4881
test | loss: 1.6453 | accuracy: 0.4935

epoch: 13
train | loss: 1.4447 | accuracy: 0.4895
test | loss: 1.6607 | accuracy: 0.4982

epoch: 14
train | loss: 1.4403 | accuracy: 0.4887
test | loss: 1.6720 | accuracy: 0.5024

epoch: 15
train | loss: 1.4257 | accuracy: 0.4962
test | loss: 1.6801 | accuracy: 0.5091

epoch: 16
train | loss: 1.4159 | accuracy: 0.5015
test | loss: 1.6776 | accuracy: 0.5113

epoch: 17
train | loss: 1.3946 | accuracy: 0.5084
test | loss: 1.6888 | accuracy: 0.5108

epoch: 18
train | loss: 1.3796 | accuracy: 0.5139
test | loss: 1.6873 | accuracy: 0.5197

epoch: 19
train | loss: 1.3512 | accuracy: 0.5231
test | loss: 1.6850 | accuracy: 0.5228

epoch: 20
train | loss: 1.3152 | accuracy: 0.5369
test | loss: 1.6847 | accuracy: 0.5250

epoch: 21
train | loss: 1.2902 | accuracy: 0.5431
test | loss: 1.6922 | accuracy: 0.5277

epoch: 22
train | loss: 1.2762 | accuracy: 0.5508
test | loss: 1.6851 | accuracy: 0.5224

epoch: 23
train | loss: 1.2381 | accuracy: 0.5633
test | loss: 1.6809 | accuracy: 0.5318

epoch: 24
train | loss: 1.2072 | accuracy: 0.5724
test | loss: 1.6827 | accuracy: 0.5328

epoch: 25
train | loss: 1.1869 | accuracy: 0.5791
test | loss: 1.6826 | accuracy: 0.5341

epoch: 26
train | loss: 1.1662 | accuracy: 0.5876
test | loss: 1.6914 | accuracy: 0.5388

epoch: 27
train | loss: 1.1320 | accuracy: 0.5967
test | loss: 1.6975 | accuracy: 0.5373

epoch: 28
train | loss: 1.1076 | accuracy: 0.6066
test | loss: 1.7094 | accuracy: 0.5470

epoch: 29
train | loss: 1.0816 | accuracy: 0.6173
test | loss: 1.7251 | accuracy: 0.5426

epoch: 30
train | loss: 1.0566 | accuracy: 0.6257
test | loss: 1.7327 | accuracy: 0.5421

epoch: 31
train | loss: 1.0223 | accuracy: 0.6373
test | loss: 1.7530 | accuracy: 0.5440

epoch: 32
train | loss: 1.0005 | accuracy: 0.6449
test | loss: 1.7460 | accuracy: 0.5462

epoch: 33
train | loss: 0.9785 | accuracy: 0.6548
test | loss: 1.7616 | accuracy: 0.5466

epoch: 34
train | loss: 0.9564 | accuracy: 0.6572
test | loss: 1.7774 | accuracy: 0.5499

epoch: 35
train | loss: 0.9422 | accuracy: 0.6637
test | loss: 1.7986 | accuracy: 0.5473

epoch: 36
train | loss: 0.9232 | accuracy: 0.6713
test | loss: 1.8099 | accuracy: 0.5507

epoch: 37
train | loss: 0.9034 | accuracy: 0.6803
test | loss: 1.8174 | accuracy: 0.5537

epoch: 38
train | loss: 0.8775 | accuracy: 0.6890
test | loss: 1.8339 | accuracy: 0.5557

epoch: 39
train | loss: 0.8590 | accuracy: 0.6935
test | loss: 1.8371 | accuracy: 0.5544

epoch: 40
train | loss: 0.8476 | accuracy: 0.6984
test | loss: 1.8579 | accuracy: 0.5539

epoch: 41
train | loss: 0.8194 | accuracy: 0.7106
test | loss: 1.8713 | accuracy: 0.5537

epoch: 42
train | loss: 0.8170 | accuracy: 0.7096
test | loss: 1.8907 | accuracy: 0.5521

epoch: 43
train | loss: 0.7959 | accuracy: 0.7179
test | loss: 1.8991 | accuracy: 0.5578

epoch: 44
train | loss: 0.7804 | accuracy: 0.7240
test | loss: 1.9010 | accuracy: 0.5614

epoch: 45
train | loss: 0.7647 | accuracy: 0.7286
test | loss: 1.9254 | accuracy: 0.5584

epoch: 46
train | loss: 0.7570 | accuracy: 0.7319
test | loss: 1.9351 | accuracy: 0.5627

epoch: 47
train | loss: 0.7378 | accuracy: 0.7400
test | loss: 1.9525 | accuracy: 0.5597

epoch: 48
train | loss: 0.7287 | accuracy: 0.7427
test | loss: 1.9660 | accuracy: 0.5598

epoch: 49
train | loss: 0.7091 | accuracy: 0.7496
test | loss: 1.9775 | accuracy: 0.5612

epoch: 50
train | loss: 0.7006 | accuracy: 0.7520
test | loss: 1.9918 | accuracy: 0.5598

epoch: 51
train | loss: 0.6956 | accuracy: 0.7556
test | loss: 2.0038 | accuracy: 0.5592

epoch: 52
train | loss: 0.6739 | accuracy: 0.7608
test | loss: 2.0097 | accuracy: 0.5636

epoch: 53
train | loss: 0.6721 | accuracy: 0.7631
test | loss: 2.0326 | accuracy: 0.5610

epoch: 54
train | loss: 0.6476 | accuracy: 0.7714
test | loss: 2.0380 | accuracy: 0.5589

epoch: 55
train | loss: 0.6374 | accuracy: 0.7752
test | loss: 2.0555 | accuracy: 0.5574

epoch: 56
train | loss: 0.6352 | accuracy: 0.7738
test | loss: 2.0782 | accuracy: 0.5598

epoch: 57
train | loss: 0.6206 | accuracy: 0.7803
test | loss: 2.0680 | accuracy: 0.5625

epoch: 58
train | loss: 0.6068 | accuracy: 0.7847
test | loss: 2.0922 | accuracy: 0.5584

epoch: 59
train | loss: 0.5987 | accuracy: 0.7887
test | loss: 2.1106 | accuracy: 0.5605

epoch: 60
train | loss: 0.5893 | accuracy: 0.7933
test | loss: 2.1221 | accuracy: 0.5600

epoch: 61
train | loss: 0.5912 | accuracy: 0.7920
test | loss: 2.1356 | accuracy: 0.5602

epoch: 62
train | loss: 0.5870 | accuracy: 0.7928
test | loss: 2.1352 | accuracy: 0.5639

epoch: 63
train | loss: 0.5782 | accuracy: 0.7961
test | loss: 2.1434 | accuracy: 0.5638

epoch: 64
train | loss: 0.5598 | accuracy: 0.8039
test | loss: 2.1469 | accuracy: 0.5674

epoch: 65
train | loss: 0.5590 | accuracy: 0.8033
test | loss: 2.1780 | accuracy: 0.5629

epoch: 66
train | loss: 0.5523 | accuracy: 0.8064
test | loss: 2.1716 | accuracy: 0.5624

epoch: 67
train | loss: 0.5397 | accuracy: 0.8098
test | loss: 2.1794 | accuracy: 0.5622

epoch: 68
train | loss: 0.5435 | accuracy: 0.8106
test | loss: 2.1969 | accuracy: 0.5632

epoch: 69
train | loss: 0.5333 | accuracy: 0.8145
test | loss: 2.1936 | accuracy: 0.5646

epoch: 70
train | loss: 0.5242 | accuracy: 0.8168
test | loss: 2.1933 | accuracy: 0.5665

epoch: 71
train | loss: 0.5215 | accuracy: 0.8175
test | loss: 2.2041 | accuracy: 0.5659

epoch: 72
train | loss: 0.5187 | accuracy: 0.8165
test | loss: 2.2128 | accuracy: 0.5673

epoch: 73
train | loss: 0.5126 | accuracy: 0.8214
test | loss: 2.2183 | accuracy: 0.5662

epoch: 74
train | loss: 0.5064 | accuracy: 0.8241
test | loss: 2.2246 | accuracy: 0.5662

epoch: 75
train | loss: 0.5023 | accuracy: 0.8234
test | loss: 2.2365 | accuracy: 0.5653

epoch: 76
train | loss: 0.4867 | accuracy: 0.8274
test | loss: 2.2609 | accuracy: 0.5692

epoch: 77
train | loss: 0.4888 | accuracy: 0.8293
test | loss: 2.2555 | accuracy: 0.5665

epoch: 78
train | loss: 0.4905 | accuracy: 0.8297
test | loss: 2.2572 | accuracy: 0.5659

epoch: 79
train | loss: 0.4787 | accuracy: 0.8351
test | loss: 2.2567 | accuracy: 0.5692

epoch: 80
train | loss: 0.4736 | accuracy: 0.8335
test | loss: 2.2527 | accuracy: 0.5648

epoch: 81
train | loss: 0.4695 | accuracy: 0.8364
test | loss: 2.2520 | accuracy: 0.5677

epoch: 82
train | loss: 0.4748 | accuracy: 0.8353
test | loss: 2.2777 | accuracy: 0.5705

epoch: 83
train | loss: 0.4612 | accuracy: 0.8396
