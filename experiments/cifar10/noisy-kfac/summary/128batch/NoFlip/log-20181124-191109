/h/minfanzh/noisy-K-FAC_use_all_FC/main.py
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
import numpy as np
import os

from misc.utils import get_logger, get_args, makedirs
from misc.config import process_config
from misc.data_loader import load_pytorch
from core.model import Model
from core.train import Trainer


_INPUT_DIM = {
    'fmnist': [784],
    'mnist': [784],
    'cifar10': [32, 32, 3],
    'cifar100': [32, 32, 3]
}


def main():
    tf.set_random_seed(1231)
    np.random.seed(1231)

    try:
        args = get_args()
        config = process_config(args.config)
    except:
        print("Add a config file using \'--config file_name.json\'")
        exit(1)

    makedirs(config.summary_dir)
    makedirs(config.checkpoint_dir)

    # set logger
    path = os.path.dirname(os.path.abspath(__file__))
    path1 = os.path.join(path, 'core/model.py')
    path2 = os.path.join(path, 'core/train.py')
    logger = get_logger('log', logpath=config.summary_dir+'/',
                        filepath=os.path.abspath(__file__), package_files=[path1, path2])

    logger.info(config)

    # load data
    train_loader, test_loader = load_pytorch(config)

    # define computational graph
    sess = tf.Session()

    model_ = Model(config, _INPUT_DIM[config.dataset], len(train_loader.dataset))
    trainer = Trainer(sess, model_, train_loader, test_loader, config, logger)

    trainer.train()
    #trainer.check_grad()

if __name__ == "__main__":
    main()

/h/minfanzh/noisy-K-FAC_use_all_FC/core/model.py
import tensorflow as tf

from ops import optimizer as opt
from ops import layer_collection as lc
from ops import sampler as sp
from network.registry import get_model
from core.base_model import BaseModel


class Model(BaseModel):
    def __init__(self, config, input_dim, n_data):
        super().__init__(config)
        self.layer_collection = lc.LayerCollection()
        self.input_dim = input_dim
        self.n_data = n_data

        self.cov_update_op = None
        self.inv_update_op = None

        self.build_model()
        self.init_optim()
        self.init_saver()

    @property
    def trainable_variables(self):
        # note: we don't train the params of BN
        vars = []
        for var in tf.trainable_variables():
            if "w" in var.name:
                vars.append(var)
        return vars

    def build_model(self):
        self.inputs = tf.placeholder(tf.float32, [None] + self.input_dim)
        self.targets = tf.placeholder(tf.int64, [None])
        self.is_training = tf.placeholder(tf.bool)
        self.n_particles = tf.placeholder(tf.int32)

        inputs = self.inputs
        net = get_model(self.config.model_name)

        self.sampler = sp.Sampler(self.config, self.n_data, self.n_particles)
        logits, l2_loss = net(inputs, self.sampler, self.is_training,
                              self.config.batch_norm, self.layer_collection,
                              self.n_particles, self.config)

        # ensemble
        logits_ = tf.reduce_mean(
            tf.reshape(logits, [self.n_particles, -1, tf.shape(logits)[-1]]), 0)
        self.acc = tf.reduce_mean(tf.cast(tf.equal(
            self.targets, tf.argmax(logits_, axis=1)), dtype=tf.float32))

        targets_ = tf.tile(self.targets, [self.n_particles])
        self.loss = tf.reduce_mean(
            tf.nn.sparse_softmax_cross_entropy_with_logits(
                labels=targets_, logits=logits))

        coeff = self.config.kl / (self.n_data * self.config.eta)
        self.total_loss = self.loss + coeff * l2_loss

    def init_optim(self):
        self.optim = opt.KFACOptimizer(var_list=self.trainable_variables,
                                       learning_rate=self.config.learning_rate,
                                       cov_ema_decay=self.config.cov_ema_decay,
                                       damping=self.config.damping,
                                       layer_collection=self.layer_collection,
                                       norm_constraint=tf.train.exponential_decay(self.config.kl_clip,
                                                                                  self.global_step_tensor,
                                                                                  390, 0.95, staircase=True),
                                       momentum=self.config.momentum)

        self.cov_update_op = self.optim.cov_update_op
        self.inv_update_op = self.optim.inv_update_op

        with tf.control_dependencies([self.inv_update_op]):
            self.var_update_op = self.sampler.update(self.layer_collection.get_blocks())

        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        with tf.control_dependencies(update_ops):
            self.train_op = self.optim.minimize(self.total_loss, global_step=self.global_step_tensor)

    def init_saver(self):
        covs_lst = []
        for i in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):
            if "u_c" in i.name or "v_c" in i.name :
                covs_lst.append(i)   # i.name if you want just a name
        var_lst = tf.trainable_variables() + covs_lst
        self.saver = tf.train.Saver(var_list=var_lst, max_to_keep=self.config.max_to_keep)


/h/minfanzh/noisy-K-FAC_use_all_FC/core/train.py
from core.base_train import BaseTrain
from tqdm import tqdm
import numpy as np

import os
import tensorflow as tf
import pickle as pickle

GRAD_CHECK_ROOT_DIR = './grad_checks_FC_KFAC'


class Trainer(BaseTrain):
    def __init__(self, sess, model, train_loader, test_loader, config, logger):
        super(Trainer, self).__init__(sess, model, config, logger)
        self.train_loader = train_loader
        self.test_loader = test_loader

        self.checkpoint_dir = './checkpoint_KFAC_FC_flip'
        self.model_name = 'model'

    def train(self):
        if self.config.reload_step > 0 :
            print('---->reloading ', self.config.reload_step)
            self.reload(self.config.reload_step, self.sess, self.model.saver)

        for cur_epoch in range(self.config.epoch):
            self.logger.info('epoch: {}'.format(int(cur_epoch)))
            self.train_epoch(cur_epoch)
            self.test_epoch()

    def train_epoch(self, cur_epoch):
        loss_list = []
        acc_list = []
        for itr, (x, y) in enumerate(tqdm(self.train_loader)):
            feed_dict = {
                self.model.inputs: x,
                self.model.targets: y,
                self.model.n_particles: self.config.train_particles
            }
            '''if itr % 20 == 0:
                acc_test_list = []
                for (x1, y1) in self.test_loader:
                    feed_dict_test = {
                        self.model.inputs: x1,
                        self.model.targets: y1,
                        self.model.is_training: False,
                        self.model.n_particles: self.config.test_particles
                    }
                    acc_test = self.sess.run([self.model.acc], feed_dict=feed_dict_test)

                    acc_test_list.append(acc_test)                

                avg_test_acc = np.mean(acc_test_list)
                self.logger.info("itr %d : test accuracy: %5.4f\n"%(itr, float(avg_test_acc)))'''

            feed_dict.update({self.model.is_training: True})
            self.sess.run([self.model.train_op], feed_dict=feed_dict)

            feed_dict.update({self.model.is_training: False})  # note: that's important
            loss, acc = self.sess.run([self.model.loss, self.model.acc], feed_dict=feed_dict)
            loss_list.append(loss)
            acc_list.append(acc)

            cur_iter = self.model.global_step_tensor.eval(self.sess)
            if cur_iter % self.config.TCov == 0:
                self.sess.run([self.model.cov_update_op], feed_dict=feed_dict)

            if cur_iter % self.config.TInv == 0:
                self.sess.run([self.model.inv_update_op, self.model.var_update_op], feed_dict=feed_dict)
        
        '''covs_lst = []
        for i in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):
            if "u_c" in i.name or "v_c" in i.name :
                covs_lst.append(i)   # i.name if you want just a name'''

        print('---->saving ', cur_epoch+self.config.reload_step)
        checkpoint_path = os.path.join(self.checkpoint_dir, self.model_name)
        self.model.saver.save(self.sess, checkpoint_path, global_step=cur_epoch+self.config.reload_step)        

        avg_loss = np.mean(loss_list)
        avg_acc = np.mean(acc_list)
        self.logger.info("train | loss: %5.4f | accuracy: %5.4f"%(float(avg_loss), float(avg_acc)))

        # summarize
        summaries_dict = dict()
        summaries_dict['train_loss'] = avg_loss
        summaries_dict['train_acc'] = avg_acc

        # summarize
        cur_iter = self.model.global_step_tensor.eval(self.sess)
        self.summarizer.summarize(cur_iter, summaries_dict=summaries_dict)

        # self.model.save(self.sess)

    def test_epoch(self):
        loss_list = []
        acc_list = []
        for (x, y) in self.test_loader:
            feed_dict = {
                self.model.inputs: x,
                self.model.targets: y,
                self.model.is_training: False,
                self.model.n_particles: self.config.test_particles
            }
            loss, acc = self.sess.run([self.model.loss, self.model.acc], feed_dict=feed_dict)
            loss_list.append(loss)
            acc_list.append(acc)

        avg_loss = np.mean(loss_list)
        avg_acc = np.mean(acc_list)
        self.logger.info("test | loss: %5.4f | accuracy: %5.4f\n"%(float(avg_loss), float(avg_acc)))

        # summarize
        summaries_dict = dict()
        summaries_dict['test_loss'] = avg_loss
        summaries_dict['test_acc'] = avg_acc

        # summarize
        cur_iter = self.model.global_step_tensor.eval(self.sess)
        self.summarizer.summarize(cur_iter, summaries_dict=summaries_dict)

    def check_grad(self):
        if self.config.reload_step >= 0 :
            print('---->reloading ', self.config.reload_step)
            self.reload(self.config.reload_step, self.sess, self.model.saver)
         
        (x, y) = next(iter(self.train_loader))

        num_samples = 600
        num_trials = 10

        opt = self.model.optim
        
        trainable_vars = tf.trainable_variables()
        gradient_step = opt.compute_gradients(self.model.total_loss, trainable_vars)

        feed_dict = {self.model.inputs: x, self.model.targets: y, self.model.is_training: True, 
                     self.model.n_particles: self.config.train_particles}

        W1_shape = [784, 512]
        W2_shape = [512, 512]
        W3_shape = [512, 512]
        W4_shape = [512, 512]
        W5_shape = [512, 10]

        W1_grad_var = np.zeros([num_trials])
        W2_grad_var = np.zeros([num_trials])
        W3_grad_var = np.zeros([num_trials])
        W4_grad_var = np.zeros([num_trials])
        W5_grad_var = np.zeros([num_trials])


        for i in range(num_trials) :
            print('Iter {}/{}'.format(i, num_trials))
            W1_grad_lst = np.zeros([num_samples,W1_shape[0],W1_shape[1]])
            W2_grad_lst = np.zeros([num_samples,W2_shape[0],W2_shape[1]])
            W3_grad_lst = np.zeros([num_samples,W3_shape[0],W3_shape[1]])
            W4_grad_lst = np.zeros([num_samples,W4_shape[0],W4_shape[1]])
            W5_grad_lst = np.zeros([num_samples,W5_shape[0],W5_shape[1]])

            for j in range(num_samples) :
                grad_W = self.sess.run(gradient_step, feed_dict=feed_dict)
                W1_grad_lst[j,:,:] = grad_W[0][0]
                W2_grad_lst[j,:,:] = grad_W[2][0]
                W3_grad_lst[j,:,:] = grad_W[4][0]
                W4_grad_lst[j,:,:] = grad_W[6][0]
                W5_grad_lst[j,:,:] = grad_W[8][0]

            W1_grad_var[i] = np.mean(np.var(W1_grad_lst, axis=0))
            W2_grad_var[i] = np.mean(np.var(W2_grad_lst, axis=0))
            W3_grad_var[i] = np.mean(np.var(W3_grad_lst, axis=0))
            W4_grad_var[i] = np.mean(np.var(W4_grad_lst, axis=0))
            W5_grad_var[i] = np.mean(np.var(W5_grad_lst, axis=0))

        print("Batch size: ",str(self.config.batch_size)," With flip: ",str(self.config.use_flip),", W1 gradients has variance: \n",W1_grad_var)
        print("Batch size: ",str(self.config.batch_size)," With flip: ",str(self.config.use_flip),", W2 gradients has variance: \n",W2_grad_var)
        print("Batch size: ",str(self.config.batch_size)," With flip: ",str(self.config.use_flip),", W3 gradients has variance: \n",W3_grad_var)
        print("Batch size: ",str(self.config.batch_size)," With flip: ",str(self.config.use_flip),", W4 gradients has variance: \n",W4_grad_var)
        print("Batch size: ",str(self.config.batch_size)," With flip: ",str(self.config.use_flip),", W5 gradients has variance: \n",W5_grad_var)


        grad_save_path = '{}/batch{}'.format(GRAD_CHECK_ROOT_DIR, self.config.batch_size)
        if not os.path.exists(grad_save_path):
            os.makedirs(grad_save_path)

        if self.config.use_flip :
            with open('{}/ptb_var_87_train_acc_flip.pkl'.format(grad_save_path), 'wb') as f2:
                pickle.dump([W1_grad_var, W2_grad_var, W3_grad_var, W4_grad_var], f2)
                print('======================save_flip_model_batch_size_{}========================='.format(self.config.batch_size))
        else :
            with open('{}/ptb_var_87_train_acc_pert.pkl'.format(grad_save_path), 'wb') as f1:
                pickle.dump([W1_grad_var, W2_grad_var, W3_grad_var, W4_grad_var], f1)
                print('======================save_pert_model_batch_size_{}========================='.format(self.config.batch_size))


    def reload(self, step, sess, saver) :
        checkpoint_path = os.path.join(self.checkpoint_dir, self.model_name)
        model_path = checkpoint_path+'-'+str(step)
        if not os.path.exists(model_path+'.meta') :
            print('------- no such checkpoint', model_path)
            return
        print('---->restoring ', step)
        saver.restore(sess, model_path)
TCov: 10
TInv: 200
batch_norm: false
batch_size: 128
checkpoint_dir: ./experiments/cifar10/noisy-kfac/checkpoint/
cov_ema_decay: 0.99
damping: 0.001
data_aug: true
data_path: ./data
dataset: cifar10
epoch: 180
eta: 0.1
exp_name: noisy-kfac
fisher_approx: kron
fix_batch: false
kl: 0.2
kl_clip: 0.001
learning_rate: 0.01
max_to_keep: 0
model_name: vgg16
momentum: 0.9
num_workers: 2
optimizer: kfac
reload_step: 0
summary_dir: ./experiments/cifar10/noisy-kfac/summary/128batch/NoFlip/
test_batch_size: 100
test_particles: 1
train_particles: 1
use_conv2d: true
use_flip: false

epoch: 0
train | loss: 2.3024 | accuracy: 0.1050
test | loss: 2.3021 | accuracy: 0.1059

epoch: 1
train | loss: 2.3013 | accuracy: 0.1117
test | loss: 2.3003 | accuracy: 0.1289

epoch: 2
train | loss: 2.2660 | accuracy: 0.1419
test | loss: 2.0893 | accuracy: 0.1929

epoch: 3
train | loss: 1.8769 | accuracy: 0.2619
test | loss: 1.6880 | accuracy: 0.3461

epoch: 4
train | loss: 1.5910 | accuracy: 0.3931
test | loss: 1.5215 | accuracy: 0.4211

epoch: 5
train | loss: 1.3747 | accuracy: 0.4914
test | loss: 1.2380 | accuracy: 0.5427

epoch: 6
train | loss: 1.1639 | accuracy: 0.5773
test | loss: 1.0979 | accuracy: 0.6024

epoch: 7
train | loss: 0.9866 | accuracy: 0.6496
test | loss: 0.9864 | accuracy: 0.6435

epoch: 8
train | loss: 0.8484 | accuracy: 0.7031
test | loss: 0.8598 | accuracy: 0.7035

epoch: 9
train | loss: 0.7295 | accuracy: 0.7507
test | loss: 0.8035 | accuracy: 0.7247

epoch: 10
train | loss: 0.6768 | accuracy: 0.7687
test | loss: 0.7456 | accuracy: 0.7442

epoch: 11
train | loss: 0.6872 | accuracy: 0.7635
test | loss: 0.7990 | accuracy: 0.7304

epoch: 12
train | loss: 0.7712 | accuracy: 0.7373
test | loss: 0.8704 | accuracy: 0.7098

epoch: 13
train | loss: 0.8115 | accuracy: 0.7230
test | loss: 0.8266 | accuracy: 0.7240

epoch: 14
train | loss: 0.7772 | accuracy: 0.7361
test | loss: 0.7879 | accuracy: 0.7383

epoch: 15
train | loss: 0.7380 | accuracy: 0.7476
test | loss: 0.7670 | accuracy: 0.7475

epoch: 16
train | loss: 0.7170 | accuracy: 0.7556
test | loss: 0.7599 | accuracy: 0.7487

epoch: 17
train | loss: 0.6875 | accuracy: 0.7641
test | loss: 0.7354 | accuracy: 0.7566

epoch: 18
train | loss: 0.6744 | accuracy: 0.7690
test | loss: 0.7615 | accuracy: 0.7535

epoch: 19
train | loss: 0.6540 | accuracy: 0.7764
test | loss: 0.6905 | accuracy: 0.7712

epoch: 20
train | loss: 0.6322 | accuracy: 0.7829
test | loss: 0.7205 | accuracy: 0.7655

epoch: 21
train | loss: 0.6239 | accuracy: 0.7865
test | loss: 0.6816 | accuracy: 0.7731

epoch: 22
train | loss: 0.5987 | accuracy: 0.7940
test | loss: 0.6853 | accuracy: 0.7775

epoch: 23
train | loss: 0.5844 | accuracy: 0.8000
test | loss: 0.6760 | accuracy: 0.7822

epoch: 24
train | loss: 0.5769 | accuracy: 0.8016
test | loss: 0.6522 | accuracy: 0.7866

epoch: 25
train | loss: 0.5654 | accuracy: 0.8053
test | loss: 0.6545 | accuracy: 0.7860

epoch: 26
train | loss: 0.5605 | accuracy: 0.8088
test | loss: 0.6379 | accuracy: 0.7961

epoch: 27
train | loss: 0.5508 | accuracy: 0.8122
test | loss: 0.6386 | accuracy: 0.7979

epoch: 28
train | loss: 0.5479 | accuracy: 0.8145
test | loss: 0.6290 | accuracy: 0.7981

epoch: 29
train | loss: 0.5404 | accuracy: 0.8150
test | loss: 0.6320 | accuracy: 0.7948

epoch: 30
train | loss: 0.5288 | accuracy: 0.8210
test | loss: 0.6096 | accuracy: 0.8082

epoch: 31
train | loss: 0.5238 | accuracy: 0.8223
test | loss: 0.6350 | accuracy: 0.7936

epoch: 32
train | loss: 0.5249 | accuracy: 0.8219
test | loss: 0.6093 | accuracy: 0.7998

epoch: 33
train | loss: 0.5135 | accuracy: 0.8272
test | loss: 0.5906 | accuracy: 0.8072

epoch: 34
train | loss: 0.5149 | accuracy: 0.8254
test | loss: 0.5972 | accuracy: 0.8118

epoch: 35
train | loss: 0.5131 | accuracy: 0.8265
test | loss: 0.6039 | accuracy: 0.8068

epoch: 36
train | loss: 0.5186 | accuracy: 0.8272
test | loss: 0.6050 | accuracy: 0.8062

epoch: 37
train | loss: 0.5116 | accuracy: 0.8294
test | loss: 0.5969 | accuracy: 0.8078

epoch: 38
train | loss: 0.5088 | accuracy: 0.8289
test | loss: 0.5859 | accuracy: 0.8064

epoch: 39
train | loss: 0.4985 | accuracy: 0.8316
test | loss: 0.5934 | accuracy: 0.8147

epoch: 40
train | loss: 0.4948 | accuracy: 0.8349
test | loss: 0.5949 | accuracy: 0.8058

epoch: 41
train | loss: 0.4844 | accuracy: 0.8354
test | loss: 0.5863 | accuracy: 0.8135

epoch: 42
train | loss: 0.4879 | accuracy: 0.8380
test | loss: 0.6019 | accuracy: 0.8161

epoch: 43
train | loss: 0.4801 | accuracy: 0.8393
test | loss: 0.5671 | accuracy: 0.8151

epoch: 44
train | loss: 0.4675 | accuracy: 0.8469
test | loss: 0.5687 | accuracy: 0.8262

epoch: 45
train | loss: 0.4641 | accuracy: 0.8452
test | loss: 0.5480 | accuracy: 0.8288

epoch: 46
train | loss: 0.4719 | accuracy: 0.8441
test | loss: 0.5754 | accuracy: 0.8230

epoch: 47
train | loss: 0.4689 | accuracy: 0.8469
test | loss: 0.5700 | accuracy: 0.8215

epoch: 48
train | loss: 0.4589 | accuracy: 0.8477
test | loss: 0.5697 | accuracy: 0.8259

epoch: 49
train | loss: 0.4594 | accuracy: 0.8501
test | loss: 0.5806 | accuracy: 0.8263

epoch: 50
train | loss: 0.4794 | accuracy: 0.8493
test | loss: 0.5418 | accuracy: 0.8277

epoch: 51
train | loss: 0.4855 | accuracy: 0.8486
test | loss: 0.5622 | accuracy: 0.8233

epoch: 52
train | loss: 0.4860 | accuracy: 0.8504
test | loss: 0.6253 | accuracy: 0.8252

epoch: 53
train | loss: 0.5066 | accuracy: 0.8511
test | loss: 0.5611 | accuracy: 0.8319

epoch: 54
train | loss: 0.5062 | accuracy: 0.8504
test | loss: 0.5965 | accuracy: 0.8316

epoch: 55
train | loss: 0.5498 | accuracy: 0.8598
test | loss: 0.5527 | accuracy: 0.8337

epoch: 56
train | loss: 0.4806 | accuracy: 0.8626
test | loss: 0.6204 | accuracy: 0.8356

epoch: 57
train | loss: 0.4776 | accuracy: 0.8671
test | loss: 0.5567 | accuracy: 0.8459

epoch: 58
train | loss: 0.4349 | accuracy: 0.8765
test | loss: 0.5508 | accuracy: 0.8425

epoch: 59
train | loss: 0.4533 | accuracy: 0.8769
test | loss: 0.5463 | accuracy: 0.8486

epoch: 60
train | loss: 0.4623 | accuracy: 0.8800
test | loss: 0.6810 | accuracy: 0.8432

epoch: 61
train | loss: 0.5017 | accuracy: 0.8774
test | loss: 0.5258 | accuracy: 0.8483

epoch: 62
train | loss: 0.4740 | accuracy: 0.8808
test | loss: 0.5327 | accuracy: 0.8450

epoch: 63
train | loss: 0.4549 | accuracy: 0.8855
test | loss: 0.5747 | accuracy: 0.8515

epoch: 64
train | loss: 0.4918 | accuracy: 0.8840
test | loss: 0.5136 | accuracy: 0.8465

epoch: 65
train | loss: 0.4546 | accuracy: 0.8825
test | loss: 0.4666 | accuracy: 0.8535

epoch: 66
train | loss: 0.3145 | accuracy: 0.8995
test | loss: 0.4845 | accuracy: 0.8555

epoch: 67
train | loss: 0.4621 | accuracy: 0.8963
test | loss: 0.5687 | accuracy: 0.8537

epoch: 68
train | loss: 0.4124 | accuracy: 0.8967
test | loss: 0.5071 | accuracy: 0.8475

epoch: 69
train | loss: 0.4836 | accuracy: 0.8947
test | loss: 0.4669 | accuracy: 0.8591

epoch: 70
train | loss: 0.4123 | accuracy: 0.8946
test | loss: 0.6115 | accuracy: 0.8508

epoch: 71
train | loss: 0.4107 | accuracy: 0.8914
test | loss: 0.6268 | accuracy: 0.8536

epoch: 72
train | loss: 0.3807 | accuracy: 0.8940
test | loss: 0.4917 | accuracy: 0.8531

epoch: 73
train | loss: 0.3716 | accuracy: 0.8964
test | loss: 0.5608 | accuracy: 0.8485

epoch: 74
train | loss: 0.4353 | accuracy: 0.8927
test | loss: 0.4835 | accuracy: 0.8551

epoch: 75
train | loss: 0.2930 | accuracy: 0.9062
test | loss: 0.4565 | accuracy: 0.8589

epoch: 76
train | loss: 0.3033 | accuracy: 0.9076
test | loss: 0.5153 | accuracy: 0.8606

epoch: 77
train | loss: 0.3035 | accuracy: 0.9072
test | loss: 0.4672 | accuracy: 0.8631

epoch: 78
train | loss: 0.3161 | accuracy: 0.9075
test | loss: 0.5312 | accuracy: 0.8574

epoch: 79
train | loss: 0.3403 | accuracy: 0.9073
test | loss: 0.4401 | accuracy: 0.8667

epoch: 80
train | loss: 0.2429 | accuracy: 0.9182
test | loss: 0.4513 | accuracy: 0.8662

epoch: 81
train | loss: 0.2640 | accuracy: 0.9172
test | loss: 0.4889 | accuracy: 0.8621

epoch: 82
train | loss: 0.2928 | accuracy: 0.9157
test | loss: 0.4386 | accuracy: 0.8707

epoch: 83
train | loss: 0.2063 | accuracy: 0.9285
test | loss: 0.4602 | accuracy: 0.8732

epoch: 84
train | loss: 0.2093 | accuracy: 0.9279
test | loss: 0.4586 | accuracy: 0.8712

epoch: 85
train | loss: 0.2223 | accuracy: 0.9266
test | loss: 0.4528 | accuracy: 0.8687

epoch: 86
train | loss: 0.3006 | accuracy: 0.9241
test | loss: 0.4853 | accuracy: 0.8662

epoch: 87
train | loss: 0.3342 | accuracy: 0.9211
test | loss: 0.5889 | accuracy: 0.8631

epoch: 88
train | loss: 0.2742 | accuracy: 0.9229
test | loss: 0.4531 | accuracy: 0.8726

epoch: 89
train | loss: 0.2597 | accuracy: 0.9270
test | loss: 0.4737 | accuracy: 0.8676

epoch: 90
train | loss: 0.2880 | accuracy: 0.9239
test | loss: 0.4642 | accuracy: 0.8661

epoch: 91
train | loss: 0.5004 | accuracy: 0.9193
test | loss: 0.5324 | accuracy: 0.8632

epoch: 92
train | loss: 0.2669 | accuracy: 0.9262
test | loss: 0.4731 | accuracy: 0.8697

epoch: 93
train | loss: 0.2420 | accuracy: 0.9237
test | loss: 0.4833 | accuracy: 0.8638

epoch: 94
train | loss: 0.3892 | accuracy: 0.9209
test | loss: 0.5110 | accuracy: 0.8633

epoch: 95
train | loss: 0.4330 | accuracy: 0.9156
test | loss: 0.4793 | accuracy: 0.8633

epoch: 96
train | loss: 0.2339 | accuracy: 0.9305
test | loss: 0.4553 | accuracy: 0.8762

epoch: 97
train | loss: 0.2007 | accuracy: 0.9312
test | loss: 0.4626 | accuracy: 0.8718

epoch: 98
train | loss: 0.2019 | accuracy: 0.9323
test | loss: 0.4467 | accuracy: 0.8750

epoch: 99
train | loss: 0.1703 | accuracy: 0.9408
test | loss: 0.4505 | accuracy: 0.8763

epoch: 100
train | loss: 0.1898 | accuracy: 0.9375
test | loss: 0.4784 | accuracy: 0.8707

epoch: 101
train | loss: 0.2239 | accuracy: 0.9329
test | loss: 0.5002 | accuracy: 0.8718

epoch: 102
train | loss: 0.2463 | accuracy: 0.9333
test | loss: 0.4256 | accuracy: 0.8892

epoch: 103
train | loss: 0.1313 | accuracy: 0.9550
test | loss: 0.4494 | accuracy: 0.8868

epoch: 104
