/h/minfanzh/noisy-K-FAC_use_all_FC/main.py
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
import numpy as np
import os

from misc.utils import get_logger, get_args, makedirs
from misc.config import process_config
from misc.data_loader import load_pytorch
from core.model import Model
from core.train import Trainer


_INPUT_DIM = {
    'fmnist': [784],
    'mnist': [784],
    'cifar10': [32, 32, 3],
    'cifar100': [32, 32, 3]
}


def main():
    tf.set_random_seed(1231)
    np.random.seed(1231)

    try:
        args = get_args()
        config = process_config(args.config)
    except:
        print("Add a config file using \'--config file_name.json\'")
        exit(1)

    makedirs(config.summary_dir)
    makedirs(config.checkpoint_dir)

    # set logger
    path = os.path.dirname(os.path.abspath(__file__))
    path1 = os.path.join(path, 'core/model.py')
    path2 = os.path.join(path, 'core/train.py')
    logger = get_logger('log', logpath=config.summary_dir+'/',
                        filepath=os.path.abspath(__file__), package_files=[path1, path2])

    logger.info(config)

    # load data
    train_loader, test_loader = load_pytorch(config)

    # define computational graph
    sess = tf.Session()

    model_ = Model(config, _INPUT_DIM[config.dataset], len(train_loader.dataset))
    trainer = Trainer(sess, model_, train_loader, test_loader, config, logger)

    trainer.train()


if __name__ == "__main__":
    main()

/h/minfanzh/noisy-K-FAC_use_all_FC/core/model.py
import tensorflow as tf

from ops import optimizer as opt
from ops import layer_collection as lc
from ops import sampler as sp
from network.registry import get_model
from core.base_model import BaseModel


class Model(BaseModel):
    def __init__(self, config, input_dim, n_data):
        super().__init__(config)
        self.layer_collection = lc.LayerCollection()
        self.input_dim = input_dim
        self.n_data = n_data

        self.cov_update_op = None
        self.inv_update_op = None

        self.build_model()
        self.init_optim()
        self.init_saver()

    @property
    def trainable_variables(self):
        # note: we don't train the params of BN
        vars = []
        for var in tf.trainable_variables():
            if "w" in var.name:
                vars.append(var)
        return vars

    def build_model(self):
        self.inputs = tf.placeholder(tf.float32, [None] + self.input_dim)
        self.targets = tf.placeholder(tf.int64, [None])
        self.is_training = tf.placeholder(tf.bool)
        self.n_particles = tf.placeholder(tf.int32)

        inputs = self.inputs
        net = get_model(self.config.model_name)

        self.sampler = sp.Sampler(self.config, self.n_data, self.n_particles)
        logits, l2_loss = net(inputs, self.sampler, self.is_training,
                              self.config.batch_norm, self.layer_collection,
                              self.n_particles)

        # ensemble
        logits_ = tf.reduce_mean(
            tf.reshape(logits, [self.n_particles, -1, tf.shape(logits)[-1]]), 0)
        self.acc = tf.reduce_mean(tf.cast(tf.equal(
            self.targets, tf.argmax(logits_, axis=1)), dtype=tf.float32))

        targets_ = tf.tile(self.targets, [self.n_particles])
        self.loss = tf.reduce_mean(
            tf.nn.sparse_softmax_cross_entropy_with_logits(
                labels=targets_, logits=logits))

        coeff = self.config.kl / (self.n_data * self.config.eta)
        self.total_loss = self.loss + coeff * l2_loss

    def init_optim(self):
        self.optim = opt.KFACOptimizer(var_list=self.trainable_variables,
                                       learning_rate=self.config.learning_rate,
                                       cov_ema_decay=self.config.cov_ema_decay,
                                       damping=self.config.damping,
                                       layer_collection=self.layer_collection,
                                       norm_constraint=tf.train.exponential_decay(self.config.kl_clip,
                                                                                  self.global_step_tensor,
                                                                                  390, 0.95, staircase=True),
                                       momentum=self.config.momentum)

        self.cov_update_op = self.optim.cov_update_op
        self.inv_update_op = self.optim.inv_update_op

        with tf.control_dependencies([self.inv_update_op]):
            self.var_update_op = self.sampler.update(self.layer_collection.get_blocks())

        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        with tf.control_dependencies(update_ops):
            self.train_op = self.optim.minimize(self.total_loss, global_step=self.global_step_tensor)

    def init_saver(self):
        self.saver = tf.train.Saver(max_to_keep=self.config.max_to_keep)


/h/minfanzh/noisy-K-FAC_use_all_FC/core/train.py
from core.base_train import BaseTrain
from tqdm import tqdm
import numpy as np

import csv


class Trainer(BaseTrain):
    def __init__(self, sess, model, train_loader, test_loader, config, logger):
        super(Trainer, self).__init__(sess, model, config, logger)
        self.train_loader = train_loader
        self.test_loader = test_loader

    def train(self):
        for cur_epoch in range(self.config.epoch):
            self.logger.info('epoch: {}'.format(int(cur_epoch)))
            self.train_epoch()
            self.test_epoch()

    def train_epoch(self):
        loss_list = []
        acc_list = []
        store_csv_data = []
        for itr, (x, y) in enumerate(tqdm(self.train_loader)):
            feed_dict = {
                self.model.inputs: x,
                self.model.targets: y,
                self.model.n_particles: self.config.train_particles
            }
            if itr % 20 == 0:
                acc_test_list = []
                for (x1, y1) in self.test_loader:
                    feed_dict_test = {
                        self.model.inputs: x1,
                        self.model.targets: y1,
                        self.model.is_training: False,
                        self.model.n_particles: self.config.test_particles
                    }
                    acc_test = self.sess.run([self.model.acc], feed_dict=feed_dict_test)

                    acc_test_list.append(acc_test)                

                avg_test_acc = np.mean(acc_test_list)
                self.logger.info("itr %d : test accuracy: %5.4f\n"%(itr, float(avg_test_acc)))
                store_csv_data.append([float(avg_test_acc)])

            feed_dict.update({self.model.is_training: True})
            self.sess.run([self.model.train_op], feed_dict=feed_dict)

            feed_dict.update({self.model.is_training: False})  # note: that's important
            loss, acc = self.sess.run([self.model.loss, self.model.acc], feed_dict=feed_dict)
            loss_list.append(loss)
            acc_list.append(acc)

            cur_iter = self.model.global_step_tensor.eval(self.sess)
            if cur_iter % self.config.TCov == 0:
                self.sess.run([self.model.cov_update_op], feed_dict=feed_dict)

            if cur_iter % self.config.TInv == 0:
                self.sess.run([self.model.inv_update_op, self.model.var_update_op], feed_dict=feed_dict)

        storeFile = open('data_bs_cifar_128.csv', 'a')
        with storeFile :
            writer = csv.writer(storeFile)
            writer.writerows(store_csv_data)

        avg_loss = np.mean(loss_list)
        avg_acc = np.mean(acc_list)
        self.logger.info("train | loss: %5.4f | accuracy: %5.4f"%(float(avg_loss), float(avg_acc)))

        # summarize
        summaries_dict = dict()
        summaries_dict['train_loss'] = avg_loss
        summaries_dict['train_acc'] = avg_acc

        # summarize
        cur_iter = self.model.global_step_tensor.eval(self.sess)
        self.summarizer.summarize(cur_iter, summaries_dict=summaries_dict)

        # self.model.save(self.sess)

    def test_epoch(self):
        loss_list = []
        acc_list = []
        for (x, y) in self.test_loader:
            feed_dict = {
                self.model.inputs: x,
                self.model.targets: y,
                self.model.is_training: False,
                self.model.n_particles: self.config.test_particles
            }
            loss, acc = self.sess.run([self.model.loss, self.model.acc], feed_dict=feed_dict)
            loss_list.append(loss)
            acc_list.append(acc)

        avg_loss = np.mean(loss_list)
        avg_acc = np.mean(acc_list)
        self.logger.info("test | loss: %5.4f | accuracy: %5.4f\n"%(float(avg_loss), float(avg_acc)))

        # summarize
        summaries_dict = dict()
        summaries_dict['test_loss'] = avg_loss
        summaries_dict['test_acc'] = avg_acc

        # summarize
        cur_iter = self.model.global_step_tensor.eval(self.sess)
        self.summarizer.summarize(cur_iter, summaries_dict=summaries_dict)

TCov: 10
TInv: 200
batch_norm: false
batch_size: 128
checkpoint_dir: ./experiments/cifar10/noisy-kfac/checkpoint/
cov_ema_decay: 0.99
damping: 0.001
data_aug: false
data_path: ../data
dataset: cifar10
epoch: 150
eta: 0.1
exp_name: noisy-kfac
fisher_approx: kron
kl: 0.5
kl_clip: 0.001
learning_rate: 0.0001
max_to_keep: 3
model_name: vgg16
momentum: 0.9
num_workers: 2
optimizer: kfac
summary_dir: ./experiments/cifar10/noisy-kfac/summary/
test_batch_size: 100
test_particles: 1
train_particles: 1

epoch: 0
itr 0 : test accuracy: 0.0997

itr 20 : test accuracy: 0.0996

itr 40 : test accuracy: 0.0997

itr 60 : test accuracy: 0.0995

itr 80 : test accuracy: 0.0997

itr 100 : test accuracy: 0.0998

itr 120 : test accuracy: 0.0998

itr 140 : test accuracy: 0.1001

itr 160 : test accuracy: 0.0998

itr 180 : test accuracy: 0.1000

itr 200 : test accuracy: 0.1014

itr 220 : test accuracy: 0.1007

itr 240 : test accuracy: 0.1024

itr 260 : test accuracy: 0.1028

itr 280 : test accuracy: 0.1004

itr 300 : test accuracy: 0.1025

itr 320 : test accuracy: 0.1031

itr 340 : test accuracy: 0.1037

itr 360 : test accuracy: 0.1014

itr 380 : test accuracy: 0.1017

train | loss: 2.3042 | accuracy: 0.1011
test | loss: 2.3030 | accuracy: 0.1025

epoch: 1
itr 0 : test accuracy: 0.1035

itr 20 : test accuracy: 0.1033

itr 40 : test accuracy: 0.1043

itr 60 : test accuracy: 0.1036

itr 80 : test accuracy: 0.1034

itr 100 : test accuracy: 0.1053

itr 120 : test accuracy: 0.1077

itr 140 : test accuracy: 0.1063

itr 160 : test accuracy: 0.1053

itr 180 : test accuracy: 0.1068

itr 200 : test accuracy: 0.1097

itr 220 : test accuracy: 0.1151

itr 240 : test accuracy: 0.1077

itr 260 : test accuracy: 0.1134

itr 280 : test accuracy: 0.1096

itr 300 : test accuracy: 0.1163

itr 320 : test accuracy: 0.1147

itr 340 : test accuracy: 0.1180

itr 360 : test accuracy: 0.1148

itr 380 : test accuracy: 0.1160

train | loss: 2.3020 | accuracy: 0.1100
test | loss: 2.3009 | accuracy: 0.1172

epoch: 2
itr 0 : test accuracy: 0.1143

itr 20 : test accuracy: 0.1158

itr 40 : test accuracy: 0.1196

itr 60 : test accuracy: 0.1173

itr 80 : test accuracy: 0.1210

itr 100 : test accuracy: 0.1275

itr 120 : test accuracy: 0.1219

itr 140 : test accuracy: 0.1247

itr 160 : test accuracy: 0.1244

itr 180 : test accuracy: 0.1285

itr 200 : test accuracy: 0.1321

itr 220 : test accuracy: 0.1307

itr 240 : test accuracy: 0.1305

itr 260 : test accuracy: 0.1356

itr 280 : test accuracy: 0.1369

itr 300 : test accuracy: 0.1366

itr 320 : test accuracy: 0.1366

itr 340 : test accuracy: 0.1434

itr 360 : test accuracy: 0.1425

itr 380 : test accuracy: 0.1411

train | loss: 2.2995 | accuracy: 0.1273
test | loss: 2.2974 | accuracy: 0.1427

epoch: 3
itr 0 : test accuracy: 0.1447

itr 20 : test accuracy: 0.1424

itr 40 : test accuracy: 0.1405

itr 60 : test accuracy: 0.1434

itr 80 : test accuracy: 0.1500

itr 100 : test accuracy: 0.1462

itr 120 : test accuracy: 0.1466

itr 140 : test accuracy: 0.1535

itr 160 : test accuracy: 0.1565

itr 180 : test accuracy: 0.1603

itr 200 : test accuracy: 0.1528

itr 220 : test accuracy: 0.1592

itr 240 : test accuracy: 0.1582

itr 260 : test accuracy: 0.1573

itr 280 : test accuracy: 0.1613

itr 300 : test accuracy: 0.1646

itr 320 : test accuracy: 0.1650

itr 340 : test accuracy: 0.1719

itr 360 : test accuracy: 0.1687

itr 380 : test accuracy: 0.1791

train | loss: 2.2950 | accuracy: 0.1554
test | loss: 2.2912 | accuracy: 0.1752

epoch: 4
itr 0 : test accuracy: 0.1716

itr 20 : test accuracy: 0.1761

itr 40 : test accuracy: 0.1718

itr 60 : test accuracy: 0.1733

itr 80 : test accuracy: 0.1783

itr 100 : test accuracy: 0.1798

itr 120 : test accuracy: 0.1875

itr 140 : test accuracy: 0.1863

itr 160 : test accuracy: 0.1869

itr 180 : test accuracy: 0.1855

itr 200 : test accuracy: 0.1913

itr 220 : test accuracy: 0.2005

itr 240 : test accuracy: 0.1848

itr 260 : test accuracy: 0.1872

itr 280 : test accuracy: 0.1906

itr 300 : test accuracy: 0.1879

itr 320 : test accuracy: 0.1956

itr 340 : test accuracy: 0.2018

itr 360 : test accuracy: 0.2039

itr 380 : test accuracy: 0.2041

train | loss: 2.2841 | accuracy: 0.1871
test | loss: 2.2705 | accuracy: 0.2067

epoch: 5
itr 0 : test accuracy: 0.1998

itr 20 : test accuracy: 0.2065

itr 40 : test accuracy: 0.2113

itr 60 : test accuracy: 0.1971

itr 80 : test accuracy: 0.2038

itr 100 : test accuracy: 0.2074

itr 120 : test accuracy: 0.2042

itr 140 : test accuracy: 0.2093

itr 160 : test accuracy: 0.2092

itr 180 : test accuracy: 0.2156

itr 200 : test accuracy: 0.2074

itr 220 : test accuracy: 0.2102

itr 240 : test accuracy: 0.2103

itr 260 : test accuracy: 0.2027

itr 280 : test accuracy: 0.2019

itr 300 : test accuracy: 0.2133

itr 320 : test accuracy: 0.2125

itr 340 : test accuracy: 0.2157

itr 360 : test accuracy: 0.2286

itr 380 : test accuracy: 0.2243

train | loss: 2.2238 | accuracy: 0.2064
test | loss: 2.1386 | accuracy: 0.2264

epoch: 6
itr 0 : test accuracy: 0.2240

itr 20 : test accuracy: 0.2266

itr 40 : test accuracy: 0.2346

itr 60 : test accuracy: 0.2255

itr 80 : test accuracy: 0.2310

itr 100 : test accuracy: 0.2324

itr 120 : test accuracy: 0.2340

itr 140 : test accuracy: 0.2374

itr 160 : test accuracy: 0.2401

itr 180 : test accuracy: 0.2496

itr 200 : test accuracy: 0.2468

itr 220 : test accuracy: 0.2535

itr 240 : test accuracy: 0.2563

itr 260 : test accuracy: 0.2482

itr 280 : test accuracy: 0.2523

itr 300 : test accuracy: 0.2500

itr 320 : test accuracy: 0.2508

itr 340 : test accuracy: 0.2642

itr 360 : test accuracy: 0.2595

itr 380 : test accuracy: 0.2638

train | loss: 2.0413 | accuracy: 0.2470
test | loss: 1.9434 | accuracy: 0.2704

epoch: 7
itr 0 : test accuracy: 0.2717

itr 20 : test accuracy: 0.2729

itr 40 : test accuracy: 0.2724

itr 60 : test accuracy: 0.2734

itr 80 : test accuracy: 0.2616

itr 100 : test accuracy: 0.2755

itr 120 : test accuracy: 0.2759

itr 140 : test accuracy: 0.2716

itr 160 : test accuracy: 0.2764

itr 180 : test accuracy: 0.2793

itr 200 : test accuracy: 0.2827

itr 220 : test accuracy: 0.2857

itr 240 : test accuracy: 0.2852

itr 260 : test accuracy: 0.2888

itr 280 : test accuracy: 0.2671

itr 300 : test accuracy: 0.2702

itr 320 : test accuracy: 0.2797

itr 340 : test accuracy: 0.2829

itr 360 : test accuracy: 0.2813

itr 380 : test accuracy: 0.2864

train | loss: 1.9195 | accuracy: 0.2781
test | loss: 1.8881 | accuracy: 0.2921

epoch: 8
itr 0 : test accuracy: 0.2867

itr 20 : test accuracy: 0.2821

itr 40 : test accuracy: 0.2837

itr 60 : test accuracy: 0.2935

itr 80 : test accuracy: 0.2703

itr 100 : test accuracy: 0.2670

itr 120 : test accuracy: 0.2711

itr 140 : test accuracy: 0.2851

itr 160 : test accuracy: 0.2834

itr 180 : test accuracy: 0.2874

itr 200 : test accuracy: 0.2845

itr 220 : test accuracy: 0.2872

itr 240 : test accuracy: 0.2821

itr 260 : test accuracy: 0.2817

itr 280 : test accuracy: 0.2713

itr 300 : test accuracy: 0.2750

itr 320 : test accuracy: 0.2731

itr 340 : test accuracy: 0.2720

itr 360 : test accuracy: 0.2854

itr 380 : test accuracy: 0.2852

train | loss: 1.9077 | accuracy: 0.2797
test | loss: 1.9120 | accuracy: 0.2846

epoch: 9
itr 0 : test accuracy: 0.2891

itr 20 : test accuracy: 0.2842

itr 40 : test accuracy: 0.2883

itr 60 : test accuracy: 0.2884

itr 80 : test accuracy: 0.2910

itr 100 : test accuracy: 0.2828

itr 120 : test accuracy: 0.2780

itr 140 : test accuracy: 0.2849

itr 160 : test accuracy: 0.2940

itr 180 : test accuracy: 0.2825

itr 200 : test accuracy: 0.2924

itr 220 : test accuracy: 0.2861

itr 240 : test accuracy: 0.2926

itr 260 : test accuracy: 0.2887

itr 280 : test accuracy: 0.3048

itr 300 : test accuracy: 0.2912

itr 320 : test accuracy: 0.2908

itr 340 : test accuracy: 0.2968

itr 360 : test accuracy: 0.2965

itr 380 : test accuracy: 0.2959

train | loss: 1.8821 | accuracy: 0.2964
test | loss: 1.8680 | accuracy: 0.3039

epoch: 10
itr 0 : test accuracy: 0.2925

itr 20 : test accuracy: 0.3039

itr 40 : test accuracy: 0.3059

itr 60 : test accuracy: 0.3058

itr 80 : test accuracy: 0.3002

itr 100 : test accuracy: 0.2989

itr 120 : test accuracy: 0.3088

itr 140 : test accuracy: 0.3086

itr 160 : test accuracy: 0.3135

itr 180 : test accuracy: 0.3177

itr 200 : test accuracy: 0.3144

itr 220 : test accuracy: 0.3128

itr 240 : test accuracy: 0.3164

itr 260 : test accuracy: 0.3135

itr 280 : test accuracy: 0.3226

itr 300 : test accuracy: 0.3128

itr 320 : test accuracy: 0.3144

itr 340 : test accuracy: 0.3149

itr 360 : test accuracy: 0.3165

itr 380 : test accuracy: 0.3198

train | loss: 1.8319 | accuracy: 0.3190
test | loss: 1.8410 | accuracy: 0.3250

epoch: 11
itr 0 : test accuracy: 0.3257

itr 20 : test accuracy: 0.3230

itr 40 : test accuracy: 0.3213

itr 60 : test accuracy: 0.3249

itr 80 : test accuracy: 0.3343

itr 100 : test accuracy: 0.3326

itr 120 : test accuracy: 0.3234

itr 140 : test accuracy: 0.3334

itr 160 : test accuracy: 0.3280

itr 180 : test accuracy: 0.3403

itr 200 : test accuracy: 0.3384

itr 220 : test accuracy: 0.3279

itr 240 : test accuracy: 0.3395

itr 260 : test accuracy: 0.3443

itr 280 : test accuracy: 0.3388

itr 300 : test accuracy: 0.3325

itr 320 : test accuracy: 0.3353

itr 340 : test accuracy: 0.3402

itr 360 : test accuracy: 0.3398

itr 380 : test accuracy: 0.3286

train | loss: 1.7887 | accuracy: 0.3418
test | loss: 1.8112 | accuracy: 0.3386

epoch: 12
itr 0 : test accuracy: 0.3407

itr 20 : test accuracy: 0.3378

itr 40 : test accuracy: 0.3465

itr 60 : test accuracy: 0.3338

itr 80 : test accuracy: 0.3425

itr 100 : test accuracy: 0.3374

itr 120 : test accuracy: 0.3309

itr 140 : test accuracy: 0.3347

itr 160 : test accuracy: 0.3348

itr 180 : test accuracy: 0.3426

itr 200 : test accuracy: 0.3405

itr 220 : test accuracy: 0.3406

itr 240 : test accuracy: 0.3512

itr 260 : test accuracy: 0.3472

itr 280 : test accuracy: 0.3361

itr 300 : test accuracy: 0.3435

itr 320 : test accuracy: 0.3402

itr 340 : test accuracy: 0.3450

itr 360 : test accuracy: 0.3433

itr 380 : test accuracy: 0.3505

train | loss: 1.7491 | accuracy: 0.3549
test | loss: 1.7845 | accuracy: 0.3534

epoch: 13
itr 0 : test accuracy: 0.3511

itr 20 : test accuracy: 0.3498

itr 40 : test accuracy: 0.3570

itr 60 : test accuracy: 0.3538

itr 80 : test accuracy: 0.3514

itr 100 : test accuracy: 0.3504

itr 120 : test accuracy: 0.3495

itr 140 : test accuracy: 0.3527

itr 160 : test accuracy: 0.3586

itr 180 : test accuracy: 0.3502

itr 200 : test accuracy: 0.3576

itr 220 : test accuracy: 0.3608

itr 240 : test accuracy: 0.3647

itr 260 : test accuracy: 0.3578

itr 280 : test accuracy: 0.3585

itr 300 : test accuracy: 0.3567

itr 320 : test accuracy: 0.3540

itr 340 : test accuracy: 0.3473

itr 360 : test accuracy: 0.3606

itr 380 : test accuracy: 0.3505

train | loss: 1.7075 | accuracy: 0.3735
test | loss: 1.7687 | accuracy: 0.3640

epoch: 14
itr 0 : test accuracy: 0.3587

itr 20 : test accuracy: 0.3648

itr 40 : test accuracy: 0.3613

itr 60 : test accuracy: 0.3661

itr 80 : test accuracy: 0.3629

itr 100 : test accuracy: 0.3633

itr 120 : test accuracy: 0.3617

itr 140 : test accuracy: 0.3611

itr 160 : test accuracy: 0.3541

itr 180 : test accuracy: 0.3665

itr 200 : test accuracy: 0.3620

itr 220 : test accuracy: 0.3649

itr 240 : test accuracy: 0.3595

itr 260 : test accuracy: 0.3676

itr 280 : test accuracy: 0.3639

itr 300 : test accuracy: 0.3630

itr 320 : test accuracy: 0.3668

itr 340 : test accuracy: 0.3615

itr 360 : test accuracy: 0.3611

itr 380 : test accuracy: 0.3671

train | loss: 1.6801 | accuracy: 0.3831
test | loss: 1.7487 | accuracy: 0.3617

epoch: 15
itr 0 : test accuracy: 0.3607

itr 20 : test accuracy: 0.3627

itr 40 : test accuracy: 0.3662

itr 60 : test accuracy: 0.3593

itr 80 : test accuracy: 0.3690

itr 100 : test accuracy: 0.3669

itr 120 : test accuracy: 0.3698

itr 140 : test accuracy: 0.3674

itr 160 : test accuracy: 0.3699

itr 180 : test accuracy: 0.3710

itr 200 : test accuracy: 0.3635

itr 220 : test accuracy: 0.3748

itr 240 : test accuracy: 0.3705

itr 260 : test accuracy: 0.3803

itr 280 : test accuracy: 0.3704

itr 300 : test accuracy: 0.3777

itr 320 : test accuracy: 0.3690

itr 340 : test accuracy: 0.3686

itr 360 : test accuracy: 0.3737

itr 380 : test accuracy: 0.3755

train | loss: 1.6431 | accuracy: 0.3959
test | loss: 1.7291 | accuracy: 0.3766

epoch: 16
itr 0 : test accuracy: 0.3673

itr 20 : test accuracy: 0.3680

itr 40 : test accuracy: 0.3729

itr 60 : test accuracy: 0.3765

itr 80 : test accuracy: 0.3759

itr 100 : test accuracy: 0.3769

itr 120 : test accuracy: 0.3781

itr 140 : test accuracy: 0.3690

itr 160 : test accuracy: 0.3775

itr 180 : test accuracy: 0.3744

itr 200 : test accuracy: 0.3793

itr 220 : test accuracy: 0.3706

itr 240 : test accuracy: 0.3757

itr 260 : test accuracy: 0.3763

itr 280 : test accuracy: 0.3835

itr 300 : test accuracy: 0.3803

itr 320 : test accuracy: 0.3818

itr 340 : test accuracy: 0.3873

itr 360 : test accuracy: 0.3804

itr 380 : test accuracy: 0.3800

train | loss: 1.6139 | accuracy: 0.4068
test | loss: 1.7322 | accuracy: 0.3815

epoch: 17
itr 0 : test accuracy: 0.3806

itr 20 : test accuracy: 0.3765

itr 40 : test accuracy: 0.3764

itr 60 : test accuracy: 0.3860

itr 80 : test accuracy: 0.3807

itr 100 : test accuracy: 0.3807

itr 120 : test accuracy: 0.3836

itr 140 : test accuracy: 0.3817

itr 160 : test accuracy: 0.3783

itr 180 : test accuracy: 0.3801

itr 200 : test accuracy: 0.3819

itr 220 : test accuracy: 0.3773

itr 240 : test accuracy: 0.3838

itr 260 : test accuracy: 0.3830

itr 280 : test accuracy: 0.3919

itr 300 : test accuracy: 0.3895

itr 320 : test accuracy: 0.3834

itr 340 : test accuracy: 0.3871

itr 360 : test accuracy: 0.3842

itr 380 : test accuracy: 0.3890

train | loss: 1.5803 | accuracy: 0.4195
test | loss: 1.7245 | accuracy: 0.3758

epoch: 18
itr 0 : test accuracy: 0.3870

itr 20 : test accuracy: 0.3830

itr 40 : test accuracy: 0.3826

itr 60 : test accuracy: 0.3858

itr 80 : test accuracy: 0.3833

itr 100 : test accuracy: 0.3876

itr 120 : test accuracy: 0.3885

itr 140 : test accuracy: 0.3871

itr 160 : test accuracy: 0.3933

itr 180 : test accuracy: 0.3909

itr 200 : test accuracy: 0.3896

itr 220 : test accuracy: 0.3887

itr 240 : test accuracy: 0.3864

itr 260 : test accuracy: 0.3934

itr 280 : test accuracy: 0.3900

itr 300 : test accuracy: 0.3896

itr 320 : test accuracy: 0.3964

itr 340 : test accuracy: 0.3946

itr 360 : test accuracy: 0.3923

itr 380 : test accuracy: 0.3864

train | loss: 1.5482 | accuracy: 0.4320
test | loss: 1.7027 | accuracy: 0.3912

epoch: 19
itr 0 : test accuracy: 0.3899

itr 20 : test accuracy: 0.3955

itr 40 : test accuracy: 0.4006

itr 60 : test accuracy: 0.3950

itr 80 : test accuracy: 0.3912

itr 100 : test accuracy: 0.3951

